---
permalink: /
title: ""
excerpt: "Bio"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<!-- bundle exec jekyll serve -->
I am Tian Ye, a incoming Research Assistant at the HKUST (GZ), supervised by [Prof. Lei Zhu](https://sites.google.com/site/indexlzhu/home). 

Previously, I received my B.Eng degree from the Jimei University, Xiamen. I was honored to be supervised and collaborated with **Dr. Erkang Chen** and **Dr. Yun Liu** during my undergraduate studies.
My primary research interests include computer vision and deep learning, mainly focusing on image/video enhancement.

# üìù Publications
<p style='text-align: justify;'> My previous research works were dedicated to: (1) Handling visual perception problems and (2) Performing visibility enhancement, under degraded conditions, (3) Efficient neural network and its applications. </p>

<style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none !important;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none !important;
    }
    table,td,th,tr{
    	border:none !important;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    papertitle_just {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700;
    text-align: justify
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
</style>
<!-- ################################  CONTENT START  ##################################################-->
<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"> -->

<tbody>


<!-- ###################################################################################################-->
<!-- Paper V ShadowDiffusion-->
<!-- <tr onmouseout="submit23_shadowdiffusion_stop()" onmouseover="submit23_shadowdiffusion_start()" > -->
<td width="20%">
<!-- <div class="one"> -->
<!-- <div class="two" id = 'submit23_shadowdiffusion_image'><img src='./files/submit23_after.png'></div>
<img src='./files/submit23_before.png'> -->
<!-- </div> -->
<script type="text/javascript">
// function submit23_shadowdiffusion_start() {
// document.getElementById('submit23_shadowdiffusion_image').style.opacity = "1";
// }
// function submit23_shadowdiffusion_stop() {
// document.getElementById('submit23_shadowdiffusion_image').style.opacity = "0";
// }
// submit23_shadowdiffusion_stop()
</script>
</td>
<td valign="top" width="80%">
  <a href="https://arxiv.org/abs/2306.17201">
    <papertitle_just>MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling </papertitle_just>     
  </a>
  <br>
Zhengyu Zhang, Wenhao Chai, Zhongyu Jiang, <strong>Tian Ye</strong>, Mingli Song, Jenq-Neng Hwang, Gaoang Wang 
  <br>
<em>Under Review.</em>, 2023 <br>
<a href="https://arxiv.org/abs/2306.17201">PDF</a>
|
<a href="https://github.com/vvirgooo2/MPM">code</a>
<p>Treat 2D and 3D pose as two different modalities and apply three mask modeling based pretext tasks for human pose pre-training to learn spatial and temporal relations.</p>
</td>

<!-- Paper V ShadowDiffusion -->
<!-- ###################################################################################################-->
  




<!-- ###################################################################################################-->
<!-- Paper V ShadowDiffusion-->
<!-- <tr onmouseout="submit23_shadowdiffusion_stop()" onmouseover="submit23_shadowdiffusion_start()" > -->
<td width="20%">
<!-- <div class="one"> -->
<!-- <div class="two" id = 'submit23_shadowdiffusion_image'><img src='./files/submit23_after.png'></div>
<img src='./files/submit23_before.png'> -->
</td>
<td valign="top" width="80%">
  <a href="">
    <papertitle_just>Uncertainty-Driven Dynamic Degradation Perceiving and Background Modeling for Efficient Single Image Desnowing</papertitle_just>     
  </a>
  <br>
SiXiang Chen*, <strong>Tian Ye*</strong>, Chenghao Xue, Haoyu Chen, Yun Liu, Erkang Chen, Lei Zhu
  <br>
<em>ArXiv</em>, 2023 <br>
<a href="">PDF</a>
|
<a href="">code</a>
<p>Under Review.</p>
</td>

<!-- Paper V ShadowDiffusion -->
<!-- ###################################################################################################-->


<!-- ###################################################################################################-->
<!-- Paper V ShadowDiffusion-->
<!-- <tr onmouseout="submit23_shadowdiffusion_stop()" onmouseover="submit23_shadowdiffusion_start()" > -->
<td width="20%">
<!-- <div class="one"> -->
<!-- <div class="two" id = 'submit23_shadowdiffusion_image'><img src='./files/submit23_after.png'></div>
<img src='./files/submit23_before.png'> -->
</td>
<td valign="top" width="80%">
  <a href="">
    <papertitle_just>Cross-scale Prototype Learning Transformer for Image Snow Removal </papertitle_just>     
  </a>
  <br>
SiXiang Chen*, <strong>Tian Ye*</strong>, Yun Liu, Jinbin Bai, Haoyu Chen, Yunlong Lin, Jun Shi, Erkang Chen
  <br>
<em>ArXiv</em>, 2023 <br>
<a href="">PDF</a>
|
<a href="">code</a>
<p>Under Review.</p>
</td>

<!-- Paper V ShadowDiffusion -->
<!-- ###################################################################################################-->
  
<!-- ###################################################################################################-->
<!-- Paper V ShadowDiffusion-->
<!-- <tr onmouseout="submit23_shadowdiffusion_stop()" onmouseover="submit23_shadowdiffusion_start()" > -->
<td width="20%">
<!-- <div class="one"> -->
<!-- <div class="two" id = 'submit23_shadowdiffusion_image'><img src='./files/submit23_after.png'></div>
<img src='./files/submit23_before.png'> -->
<!-- </div> -->
<script type="text/javascript">
// function submit23_shadowdiffusion_start() {
// document.getElementById('submit23_shadowdiffusion_image').style.opacity = "1";
// }
// function submit23_shadowdiffusion_stop() {
// document.getElementById('submit23_shadowdiffusion_image').style.opacity = "0";
// }
// submit23_shadowdiffusion_stop()
</script>
</td>
<td valign="top" width="80%">
  <a href="">
    <papertitle_just>Sequential Affinity Learning for Video Restoration</papertitle_just>     
  </a>
  <br>
<strong>Tian Ye*</strong>, SiXiang Chen*, Yun Liu, Wenhao Chai, Jinbin Bai, Wenbin Zou, Yunchen Zhang, jiang mingchao, Erkang Chen, Chenghao Xue
  <br>
<em>ArXiv</em>, 2023 <br>
<a href="">PDF</a>
|
<a href="">code</a>
<p>Under Review.</p>
</td>

<!-- Paper V ShadowDiffusion -->
<!-- ###################################################################################################-->
  



<!-- ###################################################################################################-->
<!-- Paper V ShadowDiffusion-->
<!-- <tr onmouseout="submit23_shadowdiffusion_stop()" onmouseover="submit23_shadowdiffusion_start()" > -->
<td width="20%">
<!-- <div class="one"> -->
<!-- <div class="two" id = 'submit23_shadowdiffusion_image'><img src='./files/submit23_after.png'></div>
<img src='./files/submit23_before.png'> -->
<!-- </div> -->
<script type="text/javascript">
// function submit23_shadowdiffusion_start() {
// document.getElementById('submit23_shadowdiffusion_image').style.opacity = "1";
// }
// function submit23_shadowdiffusion_stop() {
// document.getElementById('submit23_shadowdiffusion_image').style.opacity = "0";
// }
// submit23_shadowdiffusion_stop()
</script>
</td>
<td valign="top" width="80%">
  <a href="https://arxiv.org/pdf/2305.09533.pdf">
    <papertitle_just>NightHazeFormer: Single Nighttime Haze Removal Using Prior Query Transformer</papertitle_just>     
  </a>
  <br>
Yun Liu, Zhongsheng Yan, Sixiang Chen, <strong>Tian Ye</strong>, Wenqi Ren, Erkang Chen
  <br>
<em>ArXiv</em>, 2023 <br>
<a href="https://arxiv.org/pdf/2305.09533.pdf">PDF</a>
|
<a href="">code</a>
<p>NightHazeFormer generates non-learnable prior queries that effectively guides the model to learn abundant prior features from input nighttime hazy images.</p>
</td>

<!-- Paper V ShadowDiffusion -->
<!-- ###################################################################################################-->
  


<!-- ###################################################################################################-->
<!-- Paper V ShadowDiffusion-->
<!-- <tr onmouseout="submit23_shadowdiffusion_stop()" onmouseover="submit23_shadowdiffusion_start()" > -->
<td width="20%">
<!-- <div class="one"> -->
<!-- <div class="two" id = 'submit23_shadowdiffusion_image'><img src='./files/submit23_after.png'></div>
<img src='./files/submit23_before.png'> -->
<!-- </div> -->
<script type="text/javascript">
// function submit23_shadowdiffusion_start() {
// document.getElementById('submit23_shadowdiffusion_image').style.opacity = "1";
// }
// function submit23_shadowdiffusion_stop() {
// document.getElementById('submit23_shadowdiffusion_image').style.opacity = "0";
// }
// submit23_shadowdiffusion_stop()
</script>
</td>
<td valign="top" width="80%">
  <a href="https://arxiv.org/pdf/2305.08824">
    <papertitle_just>Five A+ Network: You Only Need 9K Parameters for Underwater Image Enhancement</papertitle_just>     
  </a>
  <br>
Jingxia Jiang*, <strong>Tian Ye*</strong>, Jinbin Bai*, Sixiang Chen, Wenhao Chai, Jun Shi, Yun Liu, Erkang Chen
  <br>
<em>ArXiv</em>, 2023 <br>
<a href="https://arxiv.org/pdf/2305.08824">PDF</a>
|
<a href="https://github.com/Owen718/FiveAPlus-Network">code</a>
<p>Super-light Underwater Image Enhancement network only have ~9K parameters!</p>
</td>

<!-- Paper V ShadowDiffusion -->
<!-- ###################################################################################################-->
  



<!-- ###################################################################################################-->
<!-- Paper V ShadowDiffusion-->
<!-- <tr onmouseout="submit23_shadowdiffusion_stop()" onmouseover="submit23_shadowdiffusion_start()" > -->
<td width="20%">
<!-- <div class="one"> -->
<!-- <div class="two" id = 'submit23_shadowdiffusion_image'><img src='./files/submit23_after.png'></div>
<img src='./files/submit23_before.png'> -->
<!-- </div> -->
<script type="text/javascript">
// function submit23_shadowdiffusion_start() {
// document.getElementById('submit23_shadowdiffusion_image').style.opacity = "1";
// }
// function submit23_shadowdiffusion_stop() {
// document.getElementById('submit23_shadowdiffusion_image').style.opacity = "0";
// }
// submit23_shadowdiffusion_stop()
</script>
</td>
<td valign="top" width="80%">
  <a href="https://arxiv.org/pdf/2302.12186">
    <papertitle_just>RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement</papertitle_just>     
  </a>
  <br>
Jingxia Jiang, Jinbin Bai, Yun Liu, Junjie Yin, Sixiang Chen, <strong>Tian Ye</strong>, Eekang Chen
  <br>
<em>ICIP</em>, 2023 <br>
<a href="https://arxiv.org/pdf/2302.12186">PDF</a>
|
<a href="">code</a>
<p>we propose a Real-time Spatial and Frequency Domains Modulation Network for the efficient enhancement of colors and details in underwater images. </p>
</td>

<!-- Paper V ShadowDiffusion -->
<!-- ###################################################################################################-->
  





<!-- ###################################################################################################-->
<!-- Paper V ShadowDiffusion-->
<!-- <tr onmouseout="submit23_shadowdiffusion_stop()" onmouseover="submit23_shadowdiffusion_start()" > -->
<td width="20%">
<!-- <div class="one"> -->
<!-- <div class="two" id = 'submit23_shadowdiffusion_image'><img src='./files/submit23_after.png'></div>
<img src='./files/submit23_before.png'> -->
<!-- </div> -->
<script type="text/javascript">
// function submit23_shadowdiffusion_start() {
// document.getElementById('submit23_shadowdiffusion_image').style.opacity = "1";
// }
// function submit23_shadowdiffusion_stop() {
// document.getElementById('submit23_shadowdiffusion_image').style.opacity = "0";
// }
// submit23_shadowdiffusion_stop()
</script>
</td>
<td valign="top" width="80%">
  <a href="https://ieeexplore.ieee.org/abstract/document/10096828">
    <papertitle_just>Real-time Transformer for Depth Estimation and Haze Removal from Varicolored Haze Scenes</papertitle_just>     
  </a>
  <br>
Sixiang Chen*, <strong>Tian Ye*</strong>, Jun Shi, Yun Liu, JingXia Jiang, Erkang Chen, Peng Chen 
  <br>
<em>ICASSP Poster</em>, 2022 <br>
<a href="https://ieeexplore.ieee.org/abstract/document/10096828">PDF</a>
|
<a href="">code</a>
<p>First Real-time Transformer performs Depth Estimation and Haze Removal from Varicolored Haze Scenes.</p>
</td>

<!-- Paper V ShadowDiffusion -->
<!-- ###################################################################################################-->
  
<!-- ###################################################################################################-->
<!-- Paper IV Reflectance, AAAI'23 -->
<!-- <tr onmouseout="aaai23_reflectance_stop()" onmouseover="aaai23_reflectance_start()" > -->
<td width="20%">
<!-- <div class="one"> -->
<!-- <div class="two" id = 'aaai23_reflectance_image'><img src='./files/aaai23_after.jpg'></div>
<img src='./files/aaai23_before.jpg'> -->
<!-- </div> -->
<script type="text/javascript">
// function aaai23_reflectance_start() {
// document.getElementById('aaai23_reflectance_image').style.opacity = "1";
// }
// function aaai23_reflectance_stop() {
// document.getElementById('aaai23_reflectance_image').style.opacity = "0";
// }
// aaai23_reflectance_stop()
</script>
</td>
<td valign="top" width="80%">
  <a href="https://ieeexplore.ieee.org/abstract/document/10095605">
    <papertitle_just>MSP-former: Multi-scale projection transformer for single image desnowing</papertitle_just>     
  </a>
  <br>
  Sixiang Chen*, <strong>Tian Ye*</strong>, Yun Liu, Taodong Liao, Yi Ye, Erkang Chen
  <!-- <strong>Yeying Jin</strong>, Ruoteng Li, Wenhan Yang, Robby T. Tan
   -->
  <br>
<em>ICASSP Poster</em>, 2022 <br>
<a href="https://ieeexplore.ieee.org/abstract/document/10095605">PDF</a>
|
<a href="">code</a> 
<p>Efficient Multi-scale Projection Transformer for single image desnowing.</p>
</td>

<!-- Paper IV Reflectance, AAAI'23 -->
<!-- ###################################################################################################-->
  
<!-- ###################################################################################################-->
<!-- Paper IV Reflectance, AAAI'23 -->
<!-- <tr onmouseout="aaai23_reflectance_stop()" onmouseover="aaai23_reflectance_start()" > -->
<td width="20%">
<!-- <div class="one"> -->
<!-- <div class="two" id = 'aaai23_reflectance_image'><img src='./files/aaai23_after.jpg'></div>
<img src='./files/aaai23_before.jpg'> -->
<!-- </div> -->
<script type="text/javascript">
// function aaai23_reflectance_start() {
// document.getElementById('aaai23_reflectance_image').style.opacity = "1";
// }
// function aaai23_reflectance_stop() {
// document.getElementById('aaai23_reflectance_image').style.opacity = "0";
// }
// aaai23_reflectance_stop()
</script>
</td>
<td valign="top" width="80%">
  <a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Ye_Towards_Real-time_High-Definition_Image_Snow_Removal_Efficient_Pyramid_Network_with_ACCV_2022_paper.pdf">
    <papertitle_just>Towards Real-time High-Definition Image Snow Removal: Efficient Pyramid Network with Asymmetrical Encoder-decoder Architecture</papertitle_just>     
  </a>
  <br>
  <strong>Tian Ye*</strong>, Sixiang Chen*, Yun Liu, Yi Ye, Erkang Chen<!-- <strong>Yeying Jin</strong>, Ruoteng Li, Wenhan Yang, Robby T. Tan
   -->
  <br>
<em>ACCV Poster</em>, 2022 <br>
<a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Ye_Towards_Real-time_High-Definition_Image_Snow_Removal_Efficient_Pyramid_Network_with_ACCV_2022_paper.pdf">PDF</a>
|
<a href="">code</a> 
<p>First real-time network for High-Definition image snow removal.</p>
</td>

<!-- Paper IV Reflectance, AAAI'23 -->
<!-- ###################################################################################################-->

<!-- ###################################################################################################-->
<!-- Paper IV Reflectance, AAAI'23 -->
<!-- <tr onmouseout="aaai23_reflectance_stop()" onmouseover="aaai23_reflectance_start()" > -->
<td width="20%">
<!-- <div class="one"> -->
<!-- <div class="two" id = 'aaai23_reflectance_image'><img src='./files/aaai23_after.jpg'></div>
<img src='./files/aaai23_before.jpg'> -->
<!-- </div> -->
<script type="text/javascript">
// function aaai23_reflectance_start() {
// document.getElementById('aaai23_reflectance_image').style.opacity = "1";
// }
// function aaai23_reflectance_stop() {
// document.getElementById('aaai23_reflectance_image').style.opacity = "0";
// }
// aaai23_reflectance_stop()
</script>
</td>
<td valign="top" width="80%">
  <a href="https://link.springer.com/chapter/10.1007/978-3-031-19800-7_8">
    <papertitle_just>Perceiving and Modeling Density for Image Dehazing</papertitle_just>     
  </a>
  <br>
  <strong>Tian Ye*</strong>, Yunchen Zhang*, Mingchao Jiang*, Liang Chen, Yun Liu, Erkang Chen. 
  <!-- <strong>Yeying Jin</strong>, Ruoteng Li, Wenhan Yang, Robby T. Tan
   -->
  <br>
<em>ECCV Oral</em>, 2022 <br>
<a href="https://link.springer.com/chapter/10.1007/978-3-031-19800-7_8">PDF</a>
|
<a href="https://github.com/Owen718/ECCV22-Perceiving-and-Modeling-Density-for-Image-Dehazing">code</a> 
<p>We propose to perceive and model hazy density for single image dehazing.</p>
</td>

<!-- Paper IV Reflectance, AAAI'23 -->
<!-- ###################################################################################################-->

<!-- ###################################################################################################-->
<!-- Paper IV Reflectance, AAAI'23 -->
<!-- <tr onmouseout="aaai23_reflectance_stop()" onmouseover="aaai23_reflectance_start()" > -->
<td width="20%">
<!-- <div class="one"> -->
<!-- <div class="two" id = 'aaai23_reflectance_image'><img src='./files/aaai23_after.jpg'></div>
<img src='./files/aaai23_before.jpg'> -->
<!-- </div> -->
<script type="text/javascript">
// function aaai23_reflectance_start() {
// document.getElementById('aaai23_reflectance_image').style.opacity = "1";
// }
// function aaai23_reflectance_stop() {
// document.getElementById('aaai23_reflectance_image').style.opacity = "0";
// }
// aaai23_reflectance_stop()
</script>
</td>
<td valign="top" width="80%">
  <a href="https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Ye_Underwater_Light_Field_Retention_Neural_Rendering_for_Underwater_Imaging_CVPRW_2022_paper.pdf">
    <papertitle_just>Underwater Light Field Retention: Neural Rendering for Underwater Imaging</papertitle_just>     
  </a>
  <br>
  <strong>Tian Ye*</strong>, Sixiang Chen*, Yun Liu, Yi Ye, Erkang Chen, Yuche Li
  <!-- <strong>Yeying Jin</strong>, Ruoteng Li, Wenhan Yang, Robby T. Tan
   -->
  <br>
<em>CVPR Workshop Poster</em>, 2022 <br>
<a href="https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Ye_Underwater_Light_Field_Retention_Neural_Rendering_for_Underwater_Imaging_CVPRW_2022_paper.pdf">PDF</a>
|
<a href="https://github.com/Ephemeral182/UWNR">code</a> 
<p>A controlable Underwater Image Generation method based on Neural Rendering.</p>
</td>



</tbody>


# üéñ Competitions & Awards
- 2022 CVPR NAS Competition Supernet Track: Third Place Solution of Track 1 
- 2022 JMU Student Star Award  (20/19000)
  
# üßë‚Äçü§ù‚Äçüßë My Friends
- [Sixiang Chen](https://scholar.google.com/citations?user=EtljKSgAAAAJ&hl=en) @HKUST(GZ)
- [Jinbin Bai](https://noyii.github.io)@NUS
- [Wenhao Chai](http://rese1f.github.io)@UW&MSRA
- [Leitian Tao](https://taoleitian.github.io)@WHU
- [Yunchen Zhang](https://scholar.google.com/citations?user=GogMKLIAAAAJ&hl=en)@ZTE 
- [Mingchao Jiang](https://scholar.google.com/citations?user=-Br9r7-SC6cC&hl=en)@GUT
- [Wenbin Zhou](https://scholar.google.com/citations?user=dfe9NgEAAAAJ&hl=en)@SCUT
- [Ruiqi Wu](https://rq-wu.github.io)@NKU
- [Junhua Liu](http://www.junhualiu.ml)@CUHK(SZ)


# üí¨ Academic Services
- Reviewer: ACCV 2022, WACV 2022, AAAI 2023, ICCV 2023, BMVC 2023.


# üìñ Educations
- Sep'2019-Jul'2023: B.Eng (Telecommunication Engineering), Jimei University, Xiamen, China.



<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=KKPhQ-LXT8mek63h4Oa8BltFlbFsTTwZkLrrWb3wFEs&cl=ffffff&w=a"></script>